from fastapi import APIRouter, Request, Depends
from fastapi.responses import StreamingResponse
from app.models import StreamRequest
from app.services import ChatStreamService
from openai import AsyncOpenAI
from app.lifespan.connection import get_ollama

router = APIRouter()

headers = {
    # "Content-Type": "text/plain",
    "Content-Type": "text/event-stream", # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì„ì„ ëª…ì‹œ (SSE ë“±)
    "X-Accel-Buffering": "no",  # Nginxì˜ ì‘ë‹µ ë²„í¼ë§ ë°©ì§€
    "Cache-Control": "no-cache, no-transform", # ìºì‹œ ë°©ì§€ + ì¤‘ê°„ ì¥ì¹˜ê°€ ë‚´ìš© ë³€ê²½í•˜ì§€ ëª»í•˜ë„ë¡ í•¨
    "Transfer-Encoding": "chunked",  # ì‘ë‹µì„ chunk ë‹¨ìœ„ë¡œ ì „ì†¡ (ì „ì²´ í¬ê¸° ì—†ì´ë„ ê°€ëŠ¥)
    "Connection": "keep-alive" # ì—°ê²° ìœ ì§€ â†’ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ìœ ì§€
}

def get_chat_stream_service(ollama: AsyncOpenAI = Depends(get_ollama)) -> ChatStreamService:
    return ChatStreamService(ollama)

@router.post("/")
async def chat_stream(
    body: StreamRequest,
    request: Request,
    svc: ChatStreamService = Depends(get_chat_stream_service)
):

    # async def safe_generator():
    #     try:
    #         async for chunk in svc.stream(body.question, request):
    #             yield chunk
    #     except ClientDisconnect:
    #         print("ğŸ”Œ ë¼ìš°í„°: ClientDisconnect ê°ì§€")
    #     except asyncio.CancelledError:
    #         print("ğŸ”Œ ë¼ìš°í„°: ìš”ì²­ ì·¨ì†Œë¨")
    #     except Exception as e:
    #         print(f"ğŸ”Œ ë¼ìš°í„°: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ {e}")
    #         yield f"data: [ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜]\n\n".encode("utf-8")

    return StreamingResponse(
        svc.stream(body.prompt),
        headers=headers,
        media_type="text/event-stream"
    )

# @router.post("/chat/stream")
# async def stream_chat(body: AskRequest):
#     return StreamingResponse(ChatStreamService().stream(body.question), headers=headers, media_type="text/event-stream")
