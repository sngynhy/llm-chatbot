from datetime import datetime, timezone
from openai import AsyncOpenAI
from app.config import settings
import asyncio
import time

MODEL_NAME = "gemma:2b" # mistral, gemma:7b, wizard-math

class ChatStreamService:
    def __init__(self, ollama_client: AsyncOpenAI):
        self.client = ollama_client

    async def stream(self, question: str):
        # print('stream', question)
        messages = [
            {"role": "system", "content": "ê°„ë‹¨íˆ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜."},
            {"role": "user", "content": question},
        ]

        context = {"full_response": ""}

        try:
            stream = await self.client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                stream=True,
                # temperature=0.7,
                # max_tokens=1000,
            )
            async for event in stream:
                chunk = getattr(event, "chunk", event)
                if not hasattr(chunk, "choices") or not chunk.choices:
                    continue

                content = chunk.choices[0].delta.content or ""
                if not content:
                    continue

                # ë¬¸ì¥ ë ê¸°í˜¸ë“¤ ê°ì§€í•˜ì—¬ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
                if content.endswith((".", "!", "?")):
                    content += '\n'

                # print(content, end="", flush=True)
                context["full_response"] += content
                yield content.encode("utf-8")
                await asyncio.sleep(0.05)

        except Exception as e:
            print(f"ğŸ”Œ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
            yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ: {str(e)}]\n\n".encode("utf-8")
        finally:
            # ìŠ¤íŠ¸ë¦¼ ê°ì²´ê°€ ì¡´ì¬í•˜ë©´ ë‹«ê¸° ì‹œë„
            try:
                if 'stream' in locals():
                    if hasattr(stream, "aclose"):
                        await stream.aclose()
                    elif hasattr(stream, "close"):
                        maybe = stream.close()
                        if asyncio.iscoroutine(maybe):
                            await maybe
            except Exception:
                pass
            print("ğŸ”ŒğŸ”Œ ì—°ê²° ì¢…ë£Œ", context["full_response"])

    def set_messages(prompt, answer):
        now = datetime.now(timezone.utc)
        user_messages = [
            {
                "role": "user",
                "content": prompt,
                "createdAt": now,
                "isLatex": False
            }
        ]
        assistant_message = [
            {
                "role": "assistant",
                "content": answer,
                "createdAt": now,
                "isLatex": False
            }
        ]
        return user_messages + assistant_message


    # async def stream_old(self, question: str, request: Request):
    #     # print('stream', question)
    #     messages = [
    #         {"role": "system", "content": "ê°„ë‹¨íˆ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜."},
    #         {"role": "user", "content": question},
    #     ]

    #     context = {"full_response": ""}

    #     try:
    #         # AsyncOpenAI ìŠ¤íŠ¸ë¦¬ë° (ë¹„ë™ê¸°)
    #         async with self.client.chat.completions.stream(
    #             model=MODEL_NAME,
    #             messages=messages,
    #             stream=True,
    #             # temperature=0.7,
    #             # max_tokens=1000,
    #         ) as stream:
    #             async for chunk_event in stream:
    #                 print('chunk_event > ', chunk_event)

    #                 # í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ìƒíƒœ í™•ì¸
    #                 if await request.is_disconnected():
    #                     logging.info("í´ë¼ì´ì–¸íŠ¸ ì—°ê²° í•´ì œ ê°ì§€")
    #                     await stream.close()  # í•´ë‹¹ ìš”ì²­ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
    #                     break

    #                 if isinstance(chunk_event, ContentDeltaEvent):
    #                     continue

    #                 # chunk_eventì—ì„œ chunk ì¶”ì¶œ
    #                 chunk = chunk_event.chunk
    #                 if not chunk.choices:
    #                     continue

    #                 content = chunk.choices[0].delta.content or ""
    #                 if not content:
    #                     continue

    #                 # ë¬¸ì¥ ë ê¸°í˜¸ë“¤ ê°ì§€í•˜ì—¬ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ (ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
    #                 if content.endswith((".", "!", "?")):
    #                     content += '\n'

    #                 print(content, end="", flush=True)
    #                 context["full_response"] += content
    #                 yield content.encode("utf-8")
    #                 await asyncio.sleep(0.05)



    #     # except ClientDisconnect:
    #     #     print("ğŸ”Œ ClientDisconnect: ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ 1")
    #     # except ClientDisconnected:
    #     #     print("ğŸ”Œ ClientDisconnected: ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ 2")
    #     # except ClientDisconnectedError:
    #     #     print("ğŸ”Œ ClientDisconnectedError: ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ 3")
    #     # except asyncio.CancelledError:
    #     #     print("ğŸ”Œ AsyncIO ì·¨ì†Œ: ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨")
    #     except Exception as e:
    #         print(f"ğŸ”Œ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: {e}")
    #         yield f"\n\n[ì˜¤ë¥˜ ë°œìƒ: {str(e)}]\n\n".encode("utf-8")
    #     finally:
    #         print("ğŸ”Œ ì—°ê²° ì¢…ë£Œ", context["full_response"])
