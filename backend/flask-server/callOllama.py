from flask import Response, jsonify
import requests
import json
import time
from openai import OpenAI

OLLAMA_URL = "http://localhost:11434/api/generate" # Ollama API ì„œë²„
MODEL_NAME = "gemma:2b" # ì„¤ì¹˜í•œ ëª¨ë¸ ì´ë¦„ > mistral, gemma:2b,  ë“±
cache = {} # ìºì‹œ ì €ì¥ì†Œ (ì§ˆë¬¸ > ì‘ë‹µ)
headers = {
    "Content-Type": "text/event-stream", # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì„ì„ ëª…ì‹œ (SSE ë“±)
    "X-Accel-Buffering": "no",  # Nginxì˜ ì‘ë‹µ ë²„í¼ë§ ë°©ì§€
    "Cache-Control": "no-cache, no-transform", # ìºì‹œ ë°©ì§€ + ì¤‘ê°„ ì¥ì¹˜ê°€ ë‚´ìš© ë³€ê²½í•˜ì§€ ëª»í•˜ë„ë¡ í•¨
    "Transfer-Encoding": "chunked",  # ì‘ë‹µì„ chunk ë‹¨ìœ„ë¡œ ì „ì†¡ (ì „ì²´ í¬ê¸° ì—†ì´ë„ ê°€ëŠ¥)
    "Connection": "keep-alive" # ì—°ê²° ìœ ì§€ â†’ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ìœ ì§€
}

def stream_prompt_to_ollama(data):
    question = data['question']

    if not data or "question" not in data:
        return jsonify({"error": "Missing question field"}), 400
    
    if question.strip() in cache:
        # âœ… ìºì‹œì—ì„œ ê°€ì ¸ì˜´
        return Response(cache[question.strip()], content_type='text/plain')
    
    # ğŸ”„ Ollamaë¡œ ìš”ì²­ ë³´ë‚´ê¸°
    def generate():
        try:
            full_response = ''
            # with êµ¬ë¬¸: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € êµ¬ë¬¸ > ìì›(íŒŒì¼, ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë“±)ì„ ì—´ê³  ìë™ìœ¼ë¡œ ë‹«ì•„ì£¼ëŠ” ë¬¸ë²•
            # ìì› ê´€ë¦¬ ìë™í™”: ì—´ì—ˆë˜ ìì›ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹«ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ë‹«í˜
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì•ˆì „: ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ with ë¸”ë¡ì´ ëë‚˜ë©´ ì •ë¦¬ë¨
            # with requests.post(...) as res:
            #   requests.post(...)ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ HTTP ì—°ê²°ì„ ì—¶
            #   res ê°ì²´ëŠ” ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ì„ ë‹´ê³  ìˆìŒ
            #   with ë¸”ë¡ì„ ë¹ ì ¸ë‚˜ê°€ë©´ res.close()ê°€ ìë™ í˜¸ì¶œë˜ì–´ ë„¤íŠ¸ì›Œí¬ ìì›ì´ í•´ì œë¨
            with requests.post(OLLAMA_URL, json={
                "model": "",
                "prompt": f"{question}\ní•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜",
                "stream": True # Ollama ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ ì¡°ê°ìœ¼ë¡œ ì‘ë‹µ
            }, stream=True) as response:
                for line in response.iter_lines(): # ì¤„ ë‹¨ìœ„ë¡œ ì‘ë‹µì„ ì‹¤ì‹œê°„ íŒŒì‹±
                    if line:
                        chunk = json.loads(line.decode('utf-8')) # í•œ ì¤„ JSON íŒŒì‹±
                        response_piece = chunk.get("response", "")
                        full_response += response_piece
                        yield (response_piece).encode('utf-8')
                        time.sleep(0.1)
                        # yield: ë°›ì€ ì‘ë‹µ ì¡°ê°ì„ í•˜ë‚˜ì”© í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                        # chunk.get("response", ""): í…ìŠ¤íŠ¸ ì¡°ê°ë§Œ ì¶”ì¶œ

            # ìƒì„± ì™„ë£Œ í›„ ìºì‹œì— ì €ì¥
            cache[question] = full_response
            print('full_response', full_response)
        
        except Exception as e:
            print("error!!!!!!!!!")
            return jsonify({"error": str(e)}), 500   
    return Response(generate(), headers=headers, direct_passthrough=True)

# Ollamaë¥¼ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
client = OpenAI(
    base_url = 'http://localhost:11434/v1', # Ollama ì„œë²„ ì£¼ì†Œ
    api_key='ollama', # required, but unused
)

def chat_completion_with_ollama(question):

    if question in cache:
        return jsonify({"response": cache[question]})
    
    # ë©”ì‹œì§€ êµ¬ì„±: system + user
    messages = [
        # {"role": "system", "content": "You are a math teacher. Please solve the problem step by step."},
        {"role": "system", "content": "í•œêµ­ì–´ë¡œ í’€ì–´ì¤˜."},
        {"role": "user", "content": question}
    ]
    print("messages", messages)
    def generate():
        try:
            # ChatCompletion API í˜¸ì¶œ (Ollamaë¥¼ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì²˜ëŸ¼ ì‚¬ìš©)
            response = client.chat.completions.create(
                model=MODEL_NAME, # "wizard-math", MODEL_NAME
                messages=messages,
                stream=True  # ì‹¤ì‹œê°„ ì‘ë‹µ (streaming)
            )

            full_response = ''
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if (content):
                    # print(content, end="", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥
                    full_response += content
                    yield content.encode('utf-8')
                    time.sleep(0.1)

            cache[question] = full_response

        except Exception as e:
            print("error!!!!!!!!!", str(e))
            return jsonify({"error": str(e)}), 500
        
    return Response(generate(), headers=headers, direct_passthrough=True)

def stream_translate(question, answer):

    response = client.chat.completions.create(
        model="gemma:2b", # mistral, gemma:2b,
        messages=[{"role": "user", "content": f"Translate to Korean:\n{answer}"}],
        stream=True
    )

    print("ã„¹ë„ˆëŒœã…£ëŸ¬", response)

    full_response = ''
    for chunk in response:
        content = chunk.choices[0].message or ""
        if (content):
            # print('CONTENT', content, end="", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥
            full_response += content
            yield content.encode('utf-8')
            time.sleep(0.1)

    cache[question] = full_response

    print("????", response.choices[0].message)