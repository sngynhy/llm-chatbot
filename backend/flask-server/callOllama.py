from flask import Response, jsonify, make_response
import requests
import json
import time
from openai import OpenAI
from dbConnect import save_chat
from datetime import datetime, timezone

OLLAMA_URL = "http://localhost:11434/api/generate" # Ollama API ì„œë²„
MODEL_NAME = "gemma:2b" # ì„¤ì¹˜í•œ ëª¨ë¸ ì´ë¦„ > mistral, gemma:7b, wizard-math ë“±
headers = {
    "Content-Type": "text/event-stream", # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì„ì„ ëª…ì‹œ (SSE ë“±)
    "X-Accel-Buffering": "no",  # Nginxì˜ ì‘ë‹µ ë²„í¼ë§ ë°©ì§€
    "Cache-Control": "no-cache, no-transform", # ìºì‹œ ë°©ì§€ + ì¤‘ê°„ ì¥ì¹˜ê°€ ë‚´ìš© ë³€ê²½í•˜ì§€ ëª»í•˜ë„ë¡ í•¨
    "Transfer-Encoding": "chunked",  # ì‘ë‹µì„ chunk ë‹¨ìœ„ë¡œ ì „ì†¡ (ì „ì²´ í¬ê¸° ì—†ì´ë„ ê°€ëŠ¥)
    "Connection": "keep-alive" # ì—°ê²° ìœ ì§€ â†’ ìŠ¤íŠ¸ë¦¼ ì—°ê²° ìœ ì§€
}

# prompt ë°©ì‹
def stream_prompt_to_ollama(data):
    question = data['question']

    if not data or "question" not in data:
        return jsonify({"error": "Missing question field"}), 400
    
    # ğŸ”„ Ollamaë¡œ ìš”ì²­ ë³´ë‚´ê¸°
    def generate():
        try:
            full_response = ''
            # with êµ¬ë¬¸: ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € êµ¬ë¬¸ > ìì›(íŒŒì¼, ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë“±)ì„ ì—´ê³  ìë™ìœ¼ë¡œ ë‹«ì•„ì£¼ëŠ” ë¬¸ë²•
            # ìì› ê´€ë¦¬ ìë™í™”: ì—´ì—ˆë˜ ìì›ì„ ëª…ì‹œì ìœ¼ë¡œ ë‹«ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ë‹«í˜
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì•ˆì „: ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ with ë¸”ë¡ì´ ëë‚˜ë©´ ì •ë¦¬ë¨
            # with requests.post(...) as res:
            #   requests.post(...)ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ HTTP ì—°ê²°ì„ ì—¶
            #   res ê°ì²´ëŠ” ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ì„ ë‹´ê³  ìˆìŒ
            #   with ë¸”ë¡ì„ ë¹ ì ¸ë‚˜ê°€ë©´ res.close()ê°€ ìë™ í˜¸ì¶œë˜ì–´ ë„¤íŠ¸ì›Œí¬ ìì›ì´ í•´ì œë¨
            with requests.post(OLLAMA_URL, json={
                "model": "",
                "prompt": f"{question}\ní•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜",
                "stream": True # Ollama ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ ì¡°ê°ìœ¼ë¡œ ì‘ë‹µ
            }, stream=True) as response:
                for line in response.iter_lines(): # ì¤„ ë‹¨ìœ„ë¡œ ì‘ë‹µì„ ì‹¤ì‹œê°„ íŒŒì‹±
                    if line:
                        chunk = json.loads(line.decode('utf-8')) # í•œ ì¤„ JSON íŒŒì‹±
                        response_piece = chunk.get("response", "")
                        full_response += response_piece
                        yield (response_piece).encode('utf-8')
                        time.sleep(0.1)
                        # yield: ë°›ì€ ì‘ë‹µ ì¡°ê°ì„ í•˜ë‚˜ì”© í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                        # chunk.get("response", ""): í…ìŠ¤íŠ¸ ì¡°ê°ë§Œ ì¶”ì¶œ

            # ìƒì„± ì™„ë£Œ í›„ ìºì‹œì— ì €ì¥
            print('full_response', full_response)
        
        except Exception as e:
            print("error!!!!!!!!!")
            return jsonify({"error": str(e)}), 500   
    return Response(generate(), headers=headers, direct_passthrough=True)

# Ollamaë¥¼ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì •
client = OpenAI(
    base_url = "http://localhost:11434/v1", # Ollama ì„œë²„ ì£¼ì†Œ
    # base_url = 'http://localhost:11434/v1/chat/completions', # Ollama ì„œë²„ ì£¼ì†Œ
    api_key="ollama", # required, but unused
)

# completion ë°©ì‹
# - ì—­í•  ê¸°ë°˜ ëŒ€í™” ê°€ëŠ¥, ë¬¸ë§¥ ìœ ì§€ ê°€ëŠ¥
# - CoT(Chain-of-Thought) == ë…¼ë¦¬ì ì¸ ì¶”ë¡  ë‹¨ê³„ë¥¼ í•˜ë‚˜ì”© ì„œìˆ í•˜ëŠ” ë°ì— ìœ ë¦¬
def chat_completion_with_ollama(question, chat_meta, stream=True):
    # print("chat_meta", chat_meta)
    # ë©”ì‹œì§€ êµ¬ì„±: system + user
    messages = [
        # {"role": "system", "content": "You are a math teacher. Please solve the problem step by step."},
        # {"role": "system", "content": "ë‹¨ê³„ë³„ë¡œ ê°„ë‹¨íˆ í•œêµ­ì–´ë¡œ í’€ì–´ì¤˜."},
        {"role": "system", "content": "ê°„ë‹¨íˆ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì¤˜."},
        {"role": "user", "content": question}
    ]
    print('chat_completion_with_ollama', messages)

    context = { "full_response": "" }
    # Generator
    # iteratorë¥¼ ìƒì„±í•´ì£¼ëŠ” í•¨ìˆ˜
    # ì¼ë°˜ í•¨ìˆ˜ì—ì„œ return ëŒ€ì‹  yeildë¥¼ ì‚¬ìš©í•˜ë©´ generatorê°€ ë¨
    # yeildê°€ ì‘ë™ë  ê°’ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚°ì¶œ
    # ì¦‰, ê°’ì„ í•œ ë²ˆì— ë°˜í™˜í•˜ì§€ ì•Šê³ , í•„ìš”í•  ë•Œë§ˆë‹¤ ê°’ì„ 'ìƒìƒ'í•˜ì—¬ ë°˜í™˜
    # (iteratorëŠ” `__iter__()`ì™€ __next__() ë©”ì„œë“œë¥¼ í†µí•´ ìˆœíšŒ ê°€ëŠ¥í•œ ê°ì²´)
    def generate():
        try:
            print('ì‹œì‘!')
            # ChatCompletion API í˜¸ì¶œ (Ollamaë¥¼ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì²˜ëŸ¼ ì‚¬ìš©)
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                stream=stream  # ì‹¤ì‹œê°„ ì‘ë‹µ (streaming)
            )
            print('ì‘ë‹µí•˜ë¼!!', response)
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if (content):
                    # print(content, end="", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥
                    context["full_response"] += content
                    yield content.encode("utf-8")
                    time.sleep(0.05)
                    # time.sleep()ì´ í•„ìš”í•œ ì´ìœ  (0.05 ~ 0.1 ì •ë„ê°€ ì¢‹ìŒ)
                    # ë¸Œë¼ìš°ì € ë Œë”ë§ ì‹œê°„ í™•ë³´ > chunk ì‚¬ì´ì˜ ì‹œê°„ì„ í™•ë³´í•˜ì—¬ ë³´ë‹¤ ë” ì•ˆì •ì ìœ¼ë¡œ í‘œì‹œí•  ìˆ˜ ìˆë„ë¡
                    # ì„œë²„ ë¶€í•˜ ë¶„ì‚° & CPU ì ìœ ìœ¨ ë‚®ì¶¤ > ì‘ë‹µì´ ë„ˆë¬´ ë¹¨ë¦¬ ëŒë©´ ì„œë²„ëŠ” ê³„ì† ë£¨í”„ë¥¼ ëŒë©´ì„œ CPUë¥¼ ë§ì´ ì ìœ í•˜ê²Œ ë¨
        except Exception as e:
            print("ğŸ’¡ ì—ëŸ¬ ë°œìƒ >", str(e))
            # yield f"\n[ì„œë²„ ì˜¤ë¥˜]: {str(e)}\n"
            return jsonify({"error": str(e)}), 500
        finally:
            print("ğŸ”Œ ì—°ê²° ì¢…ë£Œ")

    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ DBì— ì €ì¥
    def response_wrapper():
        try:
            yield from generate()
            chat_data = {
                "chatId": chat_meta.get("chatId"),
                "userId": "user1", # chat_meta.get("userId")
                "title": chat_meta.get("title", "newChat"),
                "titleIsLatex": False,
                "messages": transform_messages(messages, context["full_response"])
            }
            save_chat(chat_data)
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    return Response(response_wrapper(), headers=headers, direct_passthrough=True)


def transform_messages(messages, answer):
    now = datetime.now(timezone.utc)
    # messagesì—ì„œ roleì´ userì¸ í•­ëª©ë§Œ ì¶”ì¶œ,
    user_messages = [
        {
            "role": msg["role"],
            "content": msg["content"],
            "createdAt": now,
            "isLatex": False
        }
        for msg in messages if msg["role"] == "user"
    ]
    # assistant ë©”ì‹œì§€ ì¶”ê°€
    assistant_message = {
        "role": "assistant",
        "content": answer,
        "createdAt": now,
        "isLatex": False
    }
    return user_messages + [assistant_message]



def stream_translate(question, answer):

    response = client.chat.completions.create(
        model="gemma:2b", # mistral, gemma:2b,
        messages=[{"role": "user", "content": f"Translate to Korean:\n{answer}"}],
        stream=True
    )

    full_response = ''
    for chunk in response:
        content = chunk.choices[0].message or ""
        if (content):
            # print('CONTENT', content, end="", flush=True)  # ì‹¤ì‹œê°„ ì¶œë ¥
            full_response += content
            yield content.encode('utf-8')
            time.sleep(0.1)