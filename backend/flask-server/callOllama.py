from flask import Response, jsonify, request
import time
from openai import OpenAI
from dbConnect import save_chat
from datetime import datetime, timezone
from werkzeug.exceptions import ClientDisconnected
from utils import is_client_connected

OLLAMA_URL = "http://localhost:11434/api/generate" # Ollama API μ„λ²„
MODEL_NAME = "gemma:2b" # μ„¤μΉν• λ¨λΈλ… > mistral, gemma:7b, wizard-math λ“±
headers = {
    "Content-Type": "text/event-stream", # μ¤νΈλ¦¬λ° λ°μ΄ν„°μ„μ„ λ…μ‹ (SSE λ“±)
    "X-Accel-Buffering": "no",  # Nginxμ μ‘λ‹µ λ²„νΌλ§ λ°©μ§€
    "Cache-Control": "no-cache, no-transform", # μΊμ‹ λ°©μ§€ + μ¤‘κ°„ μ¥μΉκ°€ λ‚΄μ© λ³€κ²½ν•μ§€ λ»ν•λ„λ΅ ν•¨
    "Transfer-Encoding": "chunked",  # μ‘λ‹µμ„ chunk λ‹¨μ„λ΅ μ „μ†΅ (μ „μ²΄ ν¬κΈ° μ—†μ΄λ„ κ°€λ¥)
    "Connection": "keep-alive" # μ—°κ²° μ μ§€ β†’ μ¤νΈλ¦Ό μ—°κ²° μ μ§€
}

# Ollamaλ¥Ό OpenAI λΌμ΄λΈλ¬λ¦¬λ΅ μ‚¬μ©ν•κΈ° μ„ν• μ„¤μ •
client = OpenAI(
    # base_url = 'http://localhost:11434/v1/chat/completions', # Ollama μ„λ²„ μ£Όμ†
    base_url = "http://localhost:11434/v1", # Ollama μ„λ²„ μ£Όμ†
    api_key="ollama", # required, but unused
)

# completion λ°©μ‹
# - μ—­ν•  κΈ°λ° λ€ν™” κ°€λ¥, λ¬Έλ§¥ μ μ§€ κ°€λ¥
# - CoT(Chain-of-Thought) == λ…Όλ¦¬μ μΈ μ¶”λ΅  λ‹¨κ³„λ¥Ό ν•λ‚μ”© μ„μ ν•λ” λ°μ— μ λ¦¬
def chat_completion_with_ollama(question, chat_meta, stream=True):
    # λ©”μ‹μ§€ κµ¬μ„±: system + user
    messages = [
        # {"role": "system", "content": "You are a math teacher. Please solve the problem step by step."},
        # {"role": "system", "content": "λ‹¨κ³„λ³„λ΅ κ°„λ‹¨ν ν•κµ­μ–΄λ΅ ν’€μ–΄μ¤."},
        {"role": "system", "content": "κ°„λ‹¨ν ν•κµ­μ–΄λ΅ μ„¤λ…ν•΄μ¤."},
        {"role": "user", "content": question}
    ]

    context = { "full_response": "" }
    # Generator()
    # iteratorλ¥Ό μƒμ„±ν•΄μ£Όλ” ν•¨μ
    # μΌλ° ν•¨μμ—μ„ return λ€μ‹  yeildλ¥Ό μ‚¬μ©ν•λ©΄ generatorκ°€ λ¨
    # yeildκ°€ μ‘λ™λ  κ°’μ„ μμ°¨μ μΌλ΅ μ‚°μ¶
    # μ¦‰, κ°’μ„ ν• λ²μ— λ°ν™ν•μ§€ μ•κ³ , ν•„μ”ν•  λ•λ§λ‹¤ κ°’μ„ 'μƒμƒ'ν•μ—¬ λ°ν™
    # (iteratorλ” `__iter__()`μ™€ __next__() λ©”μ„λ“λ¥Ό ν†µν•΄ μν κ°€λ¥ν• κ°μ²΄)
    def generate():
        try:
            # ChatCompletion API νΈμ¶ (Ollamaλ¥Ό OpenAI λΌμ΄λΈλ¬λ¦¬μ²λΌ μ‚¬μ©)
            response = client.chat.completions.create(
                model=MODEL_NAME,
                messages=messages,
                stream=stream  # μ‹¤μ‹κ°„ μ‘λ‹µ (streaming)
            )
            print('μ‘λ‹µν•λΌ!!', response)
            for chunk in response:
                # ν΄λΌμ΄μ–ΈνΈ μ—°κ²° μƒνƒ ν™•μΈ
                # if not is_client_connected():
                #     print("π” ν΄λΌμ΄μ–ΈνΈ μ—°κ²°μ΄ ν•΄μ λμ—μµλ‹λ‹¤. μ¤νΈλ¦¬λ°μ„ μ¤‘λ‹¨ν•©λ‹λ‹¤.")
                #     break
                
                content = chunk.choices[0].delta.content or ""
                if (content):
                    # λ¬Έμ¥ λ κΈ°νΈλ“¤ κ°μ§€ν•μ—¬ μ¤„λ°”κΏ μ²λ¦¬ (μ , λλ‚ν‘, λ¬Όμν‘)
                    sentence_endings = ['.', '!', '?']
                    if any(content.endswith(ending) for ending in sentence_endings):
                        content += '\n'

                    print(content, end="", flush=True)  # μ‹¤μ‹κ°„ μ¶λ ¥
                    context["full_response"] += content
                    yield content.encode("utf-8")
                    
                    # κ° μ²­ν¬ μ „μ†΅ ν›„ μ—°κ²° μƒνƒ μ¬ν™•μΈ
                    # if not is_client_connected():
                    #     print("π” μ²­ν¬ μ „μ†΅ ν›„ ν΄λΌμ΄μ–ΈνΈ μ—°κ²°μ΄ ν•΄μ λμ—μµλ‹λ‹¤.")
                    #     break
                        
                    time.sleep(0.05)
                    # time.sleep()μ΄ ν•„μ”ν• μ΄μ 
                    # λΈλΌμ°μ € λ λ”λ§ μ‹κ°„ ν™•λ³΄ > chunk μ‚¬μ΄μ μ‹κ°„μ„ ν™•λ³΄ν•μ—¬ λ³΄λ‹¤ λ” μ•μ •μ μΌλ΅ ν‘μ‹ν•  μ μλ„λ΅
                    # μ„λ²„ λ¶€ν• λ¶„μ‚° & CPU μ μ μ¨ λ‚®μ¶¤ > μ‘λ‹µμ΄ λ„λ¬΄ λΉ¨λ¦¬ λλ©΄ μ„λ²„λ” κ³„μ† λ£¨ν”„λ¥Ό λλ©΄μ„ CPUλ¥Ό λ§μ΄ μ μ ν•κ² λ¨
        except Exception as e:
            print("π’΅ μ—λ¬ λ°μƒ >", str(e))
            # yield f"\n[μ„λ²„ μ¤λ¥]: {str(e)}\n"
            return jsonify({"error": str(e)}), 500
        finally:
            print("π” μ—°κ²° μΆ…λ£")

    # heartbeat μ²΄ν¬λ¥Ό μ„ν• ν•¨μ
    # def check_client_heartbeat():
    #     """ν΄λΌμ΄μ–ΈνΈ μ—°κ²° μƒνƒλ¥Ό μ£ΌκΈ°μ μΌλ΅ ν™•μΈν•λ” ν•¨μ"""
    #     try:
    #         return is_client_connected()
    #     except Exception:
    #         return False

    # μ¤νΈλ¦¬λ° μ™„λ£ ν›„ DBμ— μ €μ¥
    def response_wrapper():
        try:
            yield from generate()
                
            print('DB μ‚½μ… μ¤€λΉ„ μ‹μ‘')
            chat_data = {
                "chatId": chat_meta.get("chatId"),
                "userId": "user1", # chat_meta.get("userId")
                "title": chat_meta.get("title", "newChat"),
                "titleIsLatex": False,
                "messages": set_messages(question, context["full_response"])
            }
            print('DB μ‚½μ… μ¤€λΉ„ λ')
            save_chat(chat_data)
        except ClientDisconnected:
            print("π” ν΄λΌμ΄μ–ΈνΈ μ—°κ²° ν•΄μ λ΅ μΈν•΄ DB μ €μ¥μ„ κ±΄λ„λλ‹λ‹¤.", context["full_response"])
        except Exception as e:
            return jsonify({"error": str(e)}), 500

    # Response κ°μ²΄ μƒμ„± μ‹ μ—°κ²° ν•΄μ  μ½λ°± μ¶”κ°€
    response = Response(response_wrapper(), headers=headers, direct_passthrough=True)
    
    # μ—°κ²°μ΄ λ‹«ν λ• νΈμ¶λ  μ½λ°± ν•¨μ
    def on_close():
        print("π” μ‘λ‹µ μ¤νΈλ¦Όμ΄ μΆ…λ£λμ—μµλ‹λ‹¤.")
    
    response.call_on_close(on_close)
    
    return response

def set_messages(question, answer):
    now = datetime.now(timezone.utc)
    user_messages = [
        {
            "role": "user",
            "content": question,
            "createdAt": now,
            "isLatex": False
        }
    ]
    assistant_message = [
        {
            "role": "assistant",
            "content": answer,
            "createdAt": now,
            "isLatex": False
        }
    ]
    return user_messages + assistant_message


# def transform_messages(messages, answer):
#     print('transform_messages', messages)
#     now = datetime.now(timezone.utc)
#     # messagesμ—μ„ roleμ΄ userμΈ ν•­λ©λ§ μ¶”μ¶,
#     user_messages = [
#         {
#             "role": msg["role"],
#             "content": msg["content"],
#             "createdAt": now,
#             "isLatex": False
#         }
#         for msg in messages if msg["role"] == "user"
#     ]
#     # assistant λ©”μ‹μ§€ μ¶”κ°€
#     assistant_message = {
#         "role": "assistant",
#         "content": answer,
#         "createdAt": now,
#         "isLatex": False
#     }
#     return user_messages + [assistant_message]


# prompt λ°©μ‹
# def stream_prompt_to_ollama(data):
#     question = data['question']

#     if not data or "question" not in data:
#         return jsonify({"error": "Missing question field"}), 400
    
#     # ν΄λΌμ΄μ–ΈνΈ μ—°κ²° μƒνƒ ν™•μΈ ν•¨μ
#     def is_client_connected():
#         try:
#             # Flaskμ λ‚΄μ¥ μ—°κ²° μƒνƒ ν™•μΈ
#             if hasattr(request, 'is_disconnected') and request.is_disconnected:
#                 return False
                
#             # request.environμ—μ„ μ—°κ²° μƒνƒ ν™•μΈ
#             if 'werkzeug.socket' in request.environ:
#                 socket = request.environ['werkzeug.socket']
#                 return not socket.closed
                
#             return True
#         except Exception:
#             return False
    
#     # π”„ Ollamaλ΅ μ”μ²­ λ³΄λ‚΄κΈ°
#     def generate():
#         try:
#             full_response = ''
#             # with κµ¬λ¬Έ: μ»¨ν…μ¤νΈ λ§¤λ‹μ € κµ¬λ¬Έ > μμ›(νμΌ, λ„¤νΈμ›ν¬ μ—°κ²° λ“±)μ„ μ—΄κ³  μλ™μΌλ΅ λ‹«μ•„μ£Όλ” λ¬Έλ²•
#             # μμ› κ΄€λ¦¬ μλ™ν™”: μ—΄μ—λ μμ›μ„ λ…μ‹μ μΌλ΅ λ‹«μ§€ μ•μ•„λ„ μλ™μΌλ΅ λ‹«ν
#             # μ—λ¬ λ°μƒ μ‹μ—λ„ μ•μ „: μμ™Έκ°€ λ°μƒν•΄λ„ with λΈ”λ΅μ΄ λλ‚λ©΄ μ •λ¦¬λ¨
#             # with requests.post(...) as res:
#             #   requests.post(...)λ” λ‚΄λ¶€μ μΌλ΅ HTTP μ—°κ²°μ„ μ—¶
#             #   res κ°μ²΄λ” μ‘λ‹µ μ¤νΈλ¦Όμ„ λ‹΄κ³  μμ
#             #   with λΈ”λ΅μ„ λΉ μ Έλ‚κ°€λ©΄ res.close()κ°€ μλ™ νΈμ¶λμ–΄ λ„¤νΈμ›ν¬ μμ›μ΄ ν•΄μ λ¨
#             with requests.post(OLLAMA_URL, json={
#                 "model": "",
#                 "prompt": f"{question}\nν•κµ­μ–΄λ΅ λ€λ‹µν•΄μ¤",
#                 "stream": True # Ollama μ„λ²„μ—μ„ λ°μ΄ν„°λ¥Ό μ‹¤μ‹κ°„ μ΅°κ°μΌλ΅ μ‘λ‹µ
#             }, stream=True) as response:
#                 for line in response.iter_lines(): # μ¤„ λ‹¨μ„λ΅ μ‘λ‹µμ„ μ‹¤μ‹κ°„ νμ‹±
#                     # ν΄λΌμ΄μ–ΈνΈ μ—°κ²° μƒνƒ ν™•μΈ
#                     if not is_client_connected():
#                         print("π” ν΄λΌμ΄μ–ΈνΈ μ—°κ²°μ΄ ν•΄μ λμ—μµλ‹λ‹¤. μ¤νΈλ¦¬λ°μ„ μ¤‘λ‹¨ν•©λ‹λ‹¤.")
#                         break
                        
#                     if line:
#                         chunk = json.loads(line.decode('utf-8')) # ν• μ¤„ JSON νμ‹±
#                         response_piece = chunk.get("response", "")
#                         full_response += response_piece
#                         yield (response_piece).encode('utf-8')
                        
#                         # κ° μ²­ν¬ μ „μ†΅ ν›„ μ—°κ²° μƒνƒ μ¬ν™•μΈ
#                         if not is_client_connected():
#                             print("π” μ²­ν¬ μ „μ†΅ ν›„ ν΄λΌμ΄μ–ΈνΈ μ—°κ²°μ΄ ν•΄μ λμ—μµλ‹λ‹¤.")
#                             break
                            
#                         time.sleep(0.1)
#                         # yield: λ°›μ€ μ‘λ‹µ μ΅°κ°μ„ ν•λ‚μ”© ν΄λΌμ΄μ–ΈνΈλ΅ μ „μ†΅
#                         # chunk.get("response", ""): ν…μ¤νΈ μ΅°κ°λ§ μ¶”μ¶

#             # μƒμ„± μ™„λ£ ν›„ μΊμ‹μ— μ €μ¥
#             print('full_response', full_response)
        
#         except ClientDisconnectedError:
#             print("π” ν΄λΌμ΄μ–ΈνΈκ°€ μ—°κ²°μ„ ν•΄μ ν–μµλ‹λ‹¤.")
#             return
#         except Exception as e:
#             print("error!!!!!!!!!")
#             return jsonify({"error": str(e)}), 500   
    
#     # Response κ°μ²΄ μƒμ„± μ‹ μ—°κ²° ν•΄μ  μ½λ°± μ¶”κ°€
#     response = Response(generate(), headers=headers, direct_passthrough=True)
    
#     # μ—°κ²°μ΄ λ‹«ν λ• νΈμ¶λ  μ½λ°± ν•¨μ
#     def on_close():
#         print("π” μ‘λ‹µ μ¤νΈλ¦Όμ΄ μΆ…λ£λμ—μµλ‹λ‹¤.")
    
#     response.call_on_close(on_close)
    
#     return response